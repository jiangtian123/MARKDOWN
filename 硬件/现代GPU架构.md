# 现代GPU架构
记录一下最近学习的GPU架构
围绕着以下几个问题开始
1. GPU是如何实现并行处理的？
2. GPU的内存模型是什么样的？
3. 一个模型进入到GPU后会经过哪些阶段？
4. GPU的深度测试都发生在哪个阶段？分别有什么作用？
5. SIMD和SIMT是什么？
6. GPC,TPC,SM,ROP都是什么？
7. shader中的if-else会降低渲染效率吗？
8. 渲染相同面积的图形，顶点数少的还是顶点数多的效率更高？为什么？
9. 现代GPU可以避开几何处理阶段吗？通过什么手段？
10. 可以在顶点着色器里删除顶点吗？
11. 线程和Warp的关系？
12. 哪些线程可以共享数据？
##  GPU是什么
GPU全称是Graphics Processing Unit，图形处理单元。它的功能最初与名字一致，是专门用于绘制图像和处理图元数据的特定芯片，后来渐渐加入了其它很多功能。本文从Tesla架构开始。
### Tesla架构的GPU
#### 各部分详解
![](https://pic3.zhimg.com/v2-3b955d24d33d0159669e2ae5e8d25d32_r.jpg)
<center>图1.Tesla架构的GPU<center>   

下面讲解以下各个部分的功能:
- Host Interface ：
	Cpu调用的图形API被解码后会存放在PushBuffer中，当Buffer塞满或者调用Clear时，就会送到Host Interface中。同时它还会从内存中Load数据进显存（顶点，纹理，各种Buffer等）
- Input Assember
	负责将顶点数据进行组装（按照顶点索引和图元类型），并搭配上属于他们的顶点属性，传给Vertex Work Distribution
- Vertex , Pixel,Compute Work Distribution：	
	负责将自己领域的工作给Tpc去做。
- TPC（Texture Processing Clusters）:
	里面有一个纹理单元和负责计算的硬件，是干活的主力军，其中还会搭配一个Shadred Memory，一个 Texture Unit，一个L1级缓存。
- Viewport/Clip/Raster/ZCull block：
	顶点着色器处理完，只是负责输出一堆裁剪坐标（还未进行透视除法）和一堆等待光栅化插值的属性，个模块就是用来做这个的，流水线中到现在位置都没有开放的编程的固定功能。
- ROP：
	负责对顶点片元处理器处理后的像素做最后的处理“深度/模板测试，颜色混合，硬件抗锯齿都在这里完成。
- L2 Cache，Memory Controller和DRAM：
	在Tesla架构有六个这样的部件，每个对应着六分之一的显存。L2Cache是Memory的缓冲区。TPC里发出的数据访问，写入请求，如果在TPC内部的L1级缓存没找到，就会到L2甚至一块的DRAM中找，和CPU的多级缓存类似。有Cache不命中的消耗。

![](https://pic4.zhimg.com/80/v2-92ec0098ff0107284729aa27e9069d8f_720w.webp)  

<center> 图2.TPC的结构图 <center>

> 和数量相关的参数不用太关心，之后不同架构各不一样，同一架构不同价格的GPU也不一样。  

- Geomtery Controller ：
	在光栅化前工作，负责顶点属性在芯片内的输入输出，几何着色器这种操作也由他控制。它仅是控制单元，具体的计算还是在SM内执行。Gometry Controller 会把最终结果送到 Viewport/......./ZCull back 模块去进行光栅化，或者通过Stream out回内存由程序员操控。
- SMC：
	负责在计算前将数据划分，打包成Warp，交给一个SM。如，顶点模块传给这个TPC  64个顶点数据，那么SMC会划分成两个Warp，交给一个SM处理，每个SM执行32个线程。除此之外，SMC还负责协调SM与共用部门Texture Unit之间的工作，以实现对外部纹理资源的访问。而显存中其他非纹理资源的读写甚至原子操作会通过ROP与外界联系。
- Texture Unit：
	一个TextureUnit包含四个纹理地址生成器和八个滤波单元。纹理单元的指令源是纹理坐标，输出是经过插值的纹理值。拿到的纹理数据会存在TexL1缓存中。
- SM：
	真正负责干活的地方
- I Cache;
	指令缓存，一个SM要做来自MSC的工作（如对32个顶点进行顶点变换）不是立即就能完成的，大量的指令会被缓存下来，分批执行。
- C cache 和Shadred Memory（共享内存）：
	一会讲GPU内存布局的时候说。
- MT（multi-threader） issue:
	SM的主管部门，负责把Warp任务拆成一条条的指令分给Core去执行。其对Warp的调度是Gpu并行的关键，它可以使多个Core（计算核心）对不同数据执行同一条指令。如32个顶点分别给32个Core执行平移操作。这就是SIMT（单指令多数据）
- SP：
	干活的主力军，进行各种数学运算，加减乘除。
- SFU:
	特殊函数单元，执行如sin，cos等函数，还有属性插值，透视矫正等。
	总结：
	TPC干的活，可能是来自着色器代码，可能是CUDA Kernel，它们都会被编译成中间语言，然后被优化器转化为而至今的GPU指令。中间指令如果是矢量(SIMD)，也会被转换为多条的Tesla SM标量指令，如4维向量相加，会被拆成4个标量相加。

SMC拿到一个着色器的所有指令后，每次会把这些指令以32个线程为单位，发给 SM，而负责执行完这个着色器的所有指令的32个线程就称为一个**WARP**。指令存在了I Cache里，Issue一次拿一个给SP或者SFU执行，一个SM只有8个SP，所以每个SP要连续干4次。

SM是以32个线程为一个Warp执行指令的，但是程序会有分支和循环，如果是动态的循环和分支，那32个线程就有可能出现每个线程执行的分支和循环不一样的局面，这时就会按照最久的那个线程的执行结束为整个Warp执行结束的标志。

如上所述，线程也会遇到内存访问，而内存访问所需要的时钟周期，比计算要慢得多。如果一个Warp进行内存访问了，这个SM就干等吗？这不是计算机的设计思路，一个黑心老板是不会让一个部门空闲的，所以每个SM以Warp为上下文，进行切换，每个Warp都有自己的寄存器，遇到内存访问时，就会暂时存下来现在执行的指令和数据，然后SM切换下一个Warp。这点和CPU的多线程很像。
#### 通用计算
上述可以看到，负责计算的元件已经被剥离出来变成了SM，一个Sm里可能执行的是VS也可能是PS，它和图形没有一点关系了。所以只利用SM做并行计算的需求就出现了，这就是通用计算管线。
在通用计算管线里，所有线程都需要程序员去分配和调用。每个线程的任务不再是固定的顶点和片元，而是任何我们想做的事。一个任务，分配多少线程，每个线程做什么工作，都有程序员说了算。
除此之外，与图形管线相比，通用管线还有一个最大的不同:我们分配的这些线程是**协作式**的，我们可以根据线程ID分配他们干不同的活，他们之间还可以进行数据传递。图形管线也会进行协作，但是都是写死的（如执行Ps的时候，是以四个线程为单位的，方便计算ddx，ddy）。
正因为通用管线释放了这么多的自由度，使得其性能的上限被提高了，我们可以根据具体任务，分配线程，并设计它们的协作模型以及数据依赖关系。不过，性能的上限是需要我们自己去探索达到的，如果缺乏对硬件的基本理解与优化技巧，深不可测的性能下限也在那等着我们。
#### CUDA和计算着色器
CUDA（Compute Unified Device Architecture）是最先被推出的，它轻装上阵彻底和图形划清了界限，成为了许多非图形学背景科研人员的并行计算平台。
后来，随着各种图形算法日益复杂，搞图形的人发现CUDA真香，也眼馋着想用上通用计算大杀器，所以各个API就相继推出了计算着色器。反正支持这套东西的底层硬件都是一样的，无非就是换了一堆名词来包装出这套一模一样的通用计算模型而已。

<img src="https://pic2.zhimg.com/80/v2-d9689b21d2fcac3045886bdfcee3f565_720w.webp" style="zoom: 67%;" />

<center> 图3.CUDA的线程组织模型 <center>
CUDA线程的三级组织结构为：Grid-Block-Thread，每个Block包含多少线程在核函数中写死，Block是协作发生的组织单位（因此也被称为CTA，cooperative thread array），里面的线程可以通过共享内存传递数据。每个Grid包含多少Block，由应用程序在每次调用时指定，同一个Grid的所有Block之间则是完全独立的，没有数据依赖。
因为线程与任务之间的映射是由我们决定的，那么我们自然需要知道每一个线程的ID，才能通过它获取到对应的数据，执行对应的计算任务，将计算结果写到对应的内存中。不过因为其三级组织结构，而且每一级结构可以支持三维的索引（上图只显示了2维），光有一个ID可能不能满足我们的需求。当然我们都可以自己算出来，但鉴于其需要被高频使用，因此不同API全都提供了一堆内置量，来描述ID的不同表示，以避免我们自己计算浪费性能。

![](https://pic3.zhimg.com/80/v2-36f03e7fa80a9530c71ad87faca8034a_720w.webp)
<center> 图4.CUDA的线程模式<center>

#### 真正的并行单元是Warp
又是三级线程组织结构，又是三维的线程ID映射，初学起来让人头晕脑胀，但是我们别忘了，无论上层的概念如何复杂，底层的硬件执行单元都是SM，真正的并行单位始终是Warp。因此优化的基础大多是以Warp为主角的，比如：
- **最好为每个Block分配Warp线程数（32）的整数倍线程数**。因为不管多少线程都要拆分成Warp单位去执行，33个线程与64个线程同样需要执行两个Warp。
- **同一分支要尽可能挤到同一Warp里**。如果设计的算法中，不同的线程不得不执行不同的分支，比如Warp1要执行分支A和B，Warp2也要执行分支A和B，如果能让Warp1只执行分支A，而Warp2只执行分支B，就能获得性能的提升。（因为每个线程要做什么都是我们说了算，因此给了我们这样的优化空间）
- **如果内存读写指令都只由一个Warp执行，那么无需同步**。因为一个Warp内的线程本身就是锁步运行的，因此肯定不需要同步。但如果是不同Warp之间存在数据依赖，则不得不同步。比如一个先写后读的常见例子：Warp1的线程要读取由Warp1其他线程写入到共享内存的数据，则无需同步，因为该数据必定已经被写入；而Warp1的线程要读取由Warp2线程写入到共享内存的数据，则必须同步，因为Warp1开始执行读取指令的时候，Warp2执行到哪里了则无法预料，因为Warp的调度是硬件决定的，对程序员是不透明的。
### GPU的内存模型

<img src="https://img2018.cnblogs.com/blog/1617944/201909/1617944-20190906001241355-608845528.png" style="zoom:150%;" />

<center> 图5.GPU管线内存交互模型<center>

<img src="https://img2018.cnblogs.com/blog/1617944/201909/1617944-20190906001709237-945454718.png" style="zoom:150%;" />
<center> 图6.GPU内存管线<center>

它们的存取速度从寄存器到系统内存依次变慢:
|存储类型|寄存器|共享内存|L1缓存|L2缓存|纹理，常量缓存|全局内存|
|:---:|:---:|:--:|:----|:---|:---:|:----:|
|访问周期|1|1~32|1~32|32~64|400~600|400~600|

- **Global Memory**
	位于显卡上的显存，被划分成数块，每块与一个L2Cache连接，能被所有的线程访问。
- **L1/L2缓存**
	L1/L2缓存（Cache）数据缓存，和CPU类似。L2为所有SM都能访问到，速度比全局内存快。L1用于存储SM内的数据，SM内的运算单元能够共享，但跨SM之间的L1不能相互访问。Telsa架构的L1缓存只能用来缓存纹理数据，但是之后的GPU架构L1缓存和Shared Memory放到了同一个物理内存，可由程序员操控。
	-**局部内存**
	局部内存是线程独享的内存资源，线程之间不能相互访问，访问速度很慢。局部内存主要是用来解决当线程的寄存器不够用的时候，存放临时数据的。
- **寄存器**
	寄存器是线程能独立访问的资源，用来存储一些线程的暂存数据。寄存器的访问速度是最快的，但是容量较小。以目前最新的Ampere架构的GA102为例，每个SM上的寄存器总量256KB，使用时被均分为了4块，且该寄存器块的64KB空间需要被warp中线程平均分配，所以在线程多的情况下，每个线程拿到的寄存器空间相当小。寄存器的分配对SM的占用率（occupancy）存在影响
	-**共享内存（Shared Memory）**
	共享内存位于SM中。访问速度很快，且对一个Block的线程是共享的。
- **常量内存（只了解移动端）**
	常量内存(constant memory) 是指存储在片下存储的设备内存上，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，常量内存为只读内存。为什么需要设立单独的常量内存？直接用global memory或者shared memory不行吗？
	主要是解决一个warp内多线程的访问相同数据的速度太慢的问题。
	当所有运算的thread都需要访问一个constant_A的常量，在存储介质上面constant_A的数据只保存了一份，而内存的物理读取方式决定了这么多thread不能在同一时刻读取到该变量，所以会出现先后访问的问题，这样使得并行计算的thread出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的cache位置产生多个副本，让thread访问时不存在冲突，从而提高并行度。
- **图像/纹理内存**
	图像/纹理（texture memory）是一种针对图形化数据的专用内存，其中texture直接翻译是纹理的意思，但根据实际的使用来看texture应该是指通常理解的1D/2D/3D结构数据，相邻数据之间存在一定关系，或者相邻数据之间需要进行相同的运算。 texture内存的构成包含 global + cache + 处理单元，texture为只读内存。texture的优势：
	texture memory 进行图像类数据加载时， warp内的thread访问的数据地址相邻，从而减少带宽的浪费。
	texture 在运算之前能进行一些处理（或者说它本身就是运算），比如聚合、映射等。
### Fermi架构

只说一下它相较于上一代GPU架构的重要改变。

Tesla架构几个问题：

- 由于CPU和GPU的带宽限制，GPU一次能拿到的顶点数量有限，使得即使SM再多也没用。
- 另一方面，顶点着色器处理完后的顶点，会先到Viewport/Clip/ZCull 单元去做剔除。如果一次加载的顶点太多，也会造成堵塞。    

Fermi架构增加了曲面细分着色器和几何着色器解决了顶点数量的限制，可以让我们在GPU内生成顶点。

![](https://pic1.zhimg.com/80/v2-82dc52b2af5388225e04fcba70d6ac9c_720w.webp)
<center> 图7.Fermi架构的GPU<center>

#### 主要变化
- 每个SM里的计算核心从八个，变成了32个
- DRAM和Memory Controlly组合还是只有六个，但是ROP从一个变成了八个。
- TPC变成了GPC
#### TPC转型成GPC
GPC通过L2Cache和显存接触，能实现大部分的GPU功能。

![](https://pic1.zhimg.com/80/v2-85750a54bd8654c5a4778c6403484c4c_720w.webp)
<center> 图8.GPC<center>   

- Tesla一个纹理单元服务两个SM，现在变成一个SM有四个纹理单元，图8中蓝色的部分。
- 被移入SM的还有PolyMorph Engine。

![](https://pic2.zhimg.com/80/v2-941edc12d44b710c7904bddbc9140219_720w.webp)

<center> 图9.PolyMorph Engine <center>

 所有的几何操作都在PolyMorph Engine中进行

- 同时每个GPC还有一个Raster Engine，服务内部的四个SM。

### 三角形的管线之旅

#### 一，从CPU到GPU

通过图形API，存储在内存中的三角形数据

- 顶点缓冲区:······、**vertex231**、**vertex232**、**vertex233**、vertex234、vertex235、vertex236、······
- 索引缓冲:······、231、232、233、231、232、236、······

通过图形API进入到显存

![](https://pic1.zhimg.com/80/v2-1207f575b64a008e909658c8cf391488_720w.webp)

图形API被驱动程序翻译成GPU可读编码，存放在Pushbuffer中。等待发射命令，由Host Interface发送给Front End

<img src="https://pic1.zhimg.com/80/v2-6cbf847e12f18647edf329a866ac2700_720w.webp" style="zoom:67%;" />

Front End将这些指令解码分类，我们从内存来到DRAM属于State类的操作：有一些会被立即执行，有一些会等到光栅化后再执行；有一些重复多余的状态设置则会被丢弃

<img src="https://pic1.zhimg.com/80/v2-377b734519bbf9484369d25a88dff10c_720w.webp" style="zoom:67%;" />

通过Drawcall命令，开始了Gpu之旅。

#### 二，图元分发

GPU通过顶点索引（如果是索引绘制），一次拿32个顶点或者32个图元，成为一个Batch。这些操作由图元分配器(Primitive Distributor)完成，顶点索引也被极限压缩，**重复的索引被剔除**。这是为了使用更少的位数来表示顶点。因为32个顶点一个Batch，比上千个顶点索引需要的位数少很多，节省内存。

<img src="https://pic1.zhimg.com/80/v2-e4849ee7e591fd7821bc70b79f771dd0_720w.webp" style="zoom:80%;" />

![](https://pic3.zhimg.com/80/v2-ec9602e257abaf1a67d10a20c16b7e2e_720w.webp)

PD将顶点或者图元Batch分发给不同的GPC。

#### 三，顶点获取（vertex Fetch）

一个个的batch被发往SM，通过SM中的PolyMorph Engine 的Vertex Fetch 把顶点数据拿到L1

Cache。

#### 四，顶点着色器

![](https://pic3.zhimg.com/80/v2-9748f581381396a4b061eab364f4d41e_720w.webp)

这个SM，相比[上一代](https://zhuanlan.zhihu.com/p/416334635)，有着诸多改进。格子间明显多了不少（不论是计算核心还是纹理单元），计算单元也变强了许多（**FMA改进了MAD，支持双精度**），**Warp调度机制也变成了双调度以满足如此繁多的元器件指令需求**。

<img src="https://pic1.zhimg.com/80/v2-1ff785a541a2692cba1c53938c5d1d34_720w.webp" style="zoom:80%;" />

每个周期中，Warp Scheduler可以将**一个或两个**Warp，发送到四个执行块中的任何两个：16CUDA核的块*2、4SFUs*2、16加载/存储单元*1。从图中可以发现，Fermi架构和Tesla架构一样，**Warp的切换周期仍然是两个处理器周期**。

之后32个顶点数据组成的Batch变成一个Warp，按照顶点着色器中的指令，进行处理。 有时候会遇到取数据的指令，这时，SM调度器就会切换其他的Warp。

#### 五，曲面细分着色器

曲面细分着色器会获取到一个个的三角形，这时三角形的新名字是Patch，在DC的时候指定，每个Patch做多可以包含32个顶点。

TCS可以在着色器中通过layout指定要输出的顶点数，而**这个输出的顶点数就是一个patch要开的线程数**，每个线程分配一个gl_InvocationID。

与顶点着色器每个线程对应一个顶点，输入输出彼此隔绝十分自闭不同，TCS自由得多，**一个patch内的所有线程可以访问这个patch的所有输入（来自顶点着色器）和输出**。这意味着可能会出现同步问题，需要用**barrier()**同步。因此，**TCS更像一个有特殊任务在身的计算着色器**。

**TCS输出的顶点只是数据**，是作为之后TES里的输入，供TES插值用的，并不代表最终的顶点数。事实上，常见的TCS没那么多戏，只是当个工具人，把顶点着色器输入的顶点属性一一对应地传给TES罢了。

身为细分控制着色器，它最重要的职责，是通过填写gl_TessLevelInner和gl_TessLevelOuter两组细分因子，**告诉接下来的Tessellator该如何细分每一个patch**。

现在三角形仍然是一堆patch，patch数没有发生变化，然而顶点可能已经面目全非。事实上，这些顶点目前只是一堆数据，**每个patch之后会被细分成多少个真正的顶点，只取决于细分因子，与TCS输出的顶点数据没有任何关系**。如果把细分因子通过API提前设置好，TCS甚至都不是细分所必须的。

<img src="https://pic1.zhimg.com/80/v2-d10d5ac52530495a36fb46e3e7c972d0_720w.webp" style="zoom:80%;" />

Work Distribution Crossbar在不同的SM之间穿梭，因为**接下来三角形的顶点数很可能急剧膨胀，当前的SM很可能承接不了**。提前根据细分因子选择一个较为空闲的SM，是个明智之举。

**Tessellator：裂变之所**

我们刚一来到这个新SM，就进入了polymorph引擎中的Tessellator。

Tessellator的工作除了需要之前TCS输出的至关重要的细分因子外，还需之后的TES中指定的各种参数：**图元生成域、细分的顶点空间划分规则、图元面部朝向、是否开启点模式**。

而其中最重要的是图元生成域，它决定了Tessellator将在什么样的**抽象patch**中插入新的点，并生成对应的图元。有三种图元生成域：**triangles、quads或isolines**。前两种最终生成三角形，后一种生成线，开启点模式，全都能画成点。

![](https://pic2.zhimg.com/80/v2-f7f622e9b67a913784cb936397331811_720w.webp)

这个抽象patch，意味着TCS原来处理的patch有多少顶点、最后输出多少顶点，它们长啥样，全都不重要。Tessellator只会在一个抽象的三角形/正方形/等值线上，根据前面所需的那些信息去插入新的顶点，最终输出的是一些0到1之间的，标记**其在抽象patch中相对位置的归一化坐标**。

> 直接复制原文了，曲面细分接触不多       

 **TES/Domain：裂变后的新生**

事实上，我们在Tessellator中并未提供任何顶点数据，我们现在仍然是之前的那副躯体。Tessellator只吐出来了一堆细分坐标gl_TessCoord，而这些就是**新顶点的种子**，它们会在我们旧的躯体上生根发芽。

随着新顶点的诞生，我**旧的躯体和灵魂（patch）也随之灰飞烟灭**了。我的新灵魂在**Task Distributor**中，由最终确定的图元类型（在这趟旅途中新图元仍然是三角形）和图元面部朝向，重新构筑成了一个全新的三角形——我的一些顶点有可能和原来的顶点位置一样，而更可能的情况是，我所有的顶点全是无中生有凭空产生的。

TES的每个线程负责一个新顶点的茁壮成长：它们可以拿到由TCS输出的，属于这个patch的所有顶点数据，**通过gl_TessCoord插值后得到新的顶点数据**。

虽然都是三角形，但，我还是原来的我吗？一切都是那么的匆匆，已没有时间让我多加思考，因为很快，我将陷入更深的迷茫和混乱之中。

#### 六，几何着色器

不要用，很费，不讲了。

#### 七，流输出

经过上面的集合阶段，顶点数据会被放回到显存中，等着下一个阶段进入GPU，或者重新读回CPU。

#### 八，顶点后处理

这个阶段，三角形会经过剔除排序等操作（仍通过Polymorph Engine）

- 视锥剔除，顶点数据再次经过图元装配，进入到GPU，进行视锥剔除

	如果完全在视锥内：我得以通过测试，幸存下来。
	如果我完全在视锥外：那毫无争议的，将被剔除。
	一半在视锥内一半在视锥外时：情况就比较尴尬了。需要视情况而定：
	- 如果和近平面相交：那么是一定要被裁剪的。理由有二：好算；不裁剪直接进行后续的透视除法会出错，整个三角形会被翻转。
	- 如果和其他面（透视投影的Frustum并不是横平竖直的）相交：裁剪则并不好算，那么原则就是，能不裁剪就不裁剪，反正不裁剪也不会有什么问题，超出视口的区域也不会被光栅化。
	- 但是如果大得离谱，导致顶点坐标大得离谱，超过了硬件的处理范围（硬件那肯定是抠抠搜搜的，坐标可不是用float来存的，也就上万，现实中的屏幕能多大它就给你设计多大，很会过日子）：那就不得不裁剪了。
- 透视除法，经过视锥剔除的三角形会做透视除法，xyz➗w，先做视锥剔除再做透视除法，是避免w为0的情况产生。然后经过视口变化，在NDC坐标中的数据，变换到了屏幕中。

#### 九，光栅化前

##### OWDX:将三角形包围盒切块，分发到对应的光栅化器中
在Fermi的GPU中，因为有多个Raster Engine，所以三角形光栅化前也会分块

<img src="https://pic1.zhimg.com/v2-b63ee123780bbd1875cdcbc7665f74d0_r.jpg" style="zoom: 80%;" />


不过，与移动端不同的是，并行处理这些块的只是Raster Engine，而不是SM，SM并不对应任何特定的块，自然也没有专属的帧缓冲，来达到移动端减少带宽的目的；当然也不延迟，来一个三角形处理一个三角形。
> 面剔除应该发生在这里

##### SWDX:排序

因为要保证按三角形提交顺序进入Raster Engine，所以在正式启程前，三角形还得先排队。虽然在ROP那里还会排序一次来保证三角形按照提交顺序输出到帧缓冲，但是在光栅化器按还得再排序一次。

<img src="https://pic1.zhimg.com/v2-5382bb711de5d04142963797cb4e55bc_r.jpg" style="zoom:80%;" />

这是为了避免有重叠的三角形出现。如上图。

##### 边设置
三角形进入Raster Engine，获取到每条边，方便后续判断一个像素是否在三角形内。
##### 光栅化

![](https://pic1.zhimg.com/80/v2-e634c3130f1fb6508e769d051763a914_720w.webp)

GPU进行光栅化时，以四个像素围成的正方形为单位。

之后会进行多重采样

##### Z-Cull

![](https://pic4.zhimg.com/80/v2-6366c72ad7da0102d4d0eedf04ad5fdb_720w.webp)
现代的GPU都会在Ps前进行一次Early-Z，来删除掉被遮挡的像素。

实际上真正的逐像素（子像素）光栅化前，还会先进行一个对块的粗糙光栅化过程。（这个块和遍历时分的块不是一个概念）

Z-Cull单元会维护每一块的深度最大值，如果我在这一块里，光栅化出来的像素最小值大于它，那意味着我必然被之前的三角形遮挡了，那这一块也就没有继续逐像素光栅化的必要了。

不过，Z-Cull只是用来节省逐像素深度测试的方法，而并不会节省后面的片元着色器，因为还有early-z挡着呢。
>early-z：整套剔除要有比较好的效果，还是需要依赖从前往后的顺序提交三角形，移动端tdbr或者z-prepass算法才能无需排序。
>
#### 十， PS着色器
进入PS之前，会先做属性插值，然后进行PS计算

#### 十一，ROP最后的部分
ROP会对像素进行**late depth test**、**模板测试**、混合等，可谓是十分忙碌。如果开启了多重采样，它们还负责最终每个像素的**resolve**，混合多个子样本的颜色得到最终的像素值。

### Kepler，MaxWell架构

Kepler架构更加节省能耗，增加了纹理单元，具备了无绑定纹理解开纹理数量的压制。

Kepler架构首次支持了**无绑定纹理特性，先API一步**（现代API就都是无绑定的了，或者说跟Shader打交道的绑定表由用户自己创，可以创好多张，就没有什么全局128的限制了），让内置的绑定表退休。

![](https://pic3.zhimg.com/80/v2-44a2020576109a5ee233542738e7ceca_720w.webp)

MaxWell支持了体素化全局光照，通过**Viewport Multicast** ，可以在VS中给三角形指定View进行投影，类似在GS中给图元选择View的操作。

### Turing架构

- 支持硬件光线追踪。
- MeshShader支持顶点避开几何阶段直接进入到光栅化。在顶点数量爆炸的时候节省更多的性能。